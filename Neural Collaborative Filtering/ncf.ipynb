{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Neural Collaborative Filtering (https://arxiv.org/pdf/1708.05031.pdf)\n",
    "- combine generalized matrix factorization, which learns linear interaction, with multi-layer perceptron, which is capable of modelling more complex & non-linear interactions\n",
    "- GMF and MLP use separate embeddings for user & item, allowing them to learn independently\n",
    "- NCF, like vanilla matrix factorization, only takes in user-item interaction data and does not consider auxiliary features of the users and items\n",
    "- and since there's only interaction data, this model is not ideal for cold start users & items"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"model_illustration.png\" style=\"width:70%\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils import data\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, f1_score, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NCF(nn.Module):\n",
    "    def __init__(self, mlp_layers, n_features, gmf_embed_dim, mlp_embed_dim, dropout=0.2):\n",
    "        super().__init__()\n",
    "        # gmf\n",
    "        self.gmf_embedding = nn.Embedding(n_features, gmf_embed_dim)\n",
    "        \n",
    "        # mlp\n",
    "        self.mlp_embedding = nn.Embedding(n_features, mlp_embed_dim)\n",
    "        # mlp input is the concatenation of user & item embedding\n",
    "        self.mlp_input_dim = 2*mlp_embed_dim\n",
    "        mlp_input_dim = self.mlp_input_dim\n",
    "        layers = []\n",
    "        for layer_dim in mlp_layers:\n",
    "            layers.append(nn.Linear(mlp_input_dim, layer_dim))\n",
    "            layers.append(nn.BatchNorm1d(layer_dim))\n",
    "            layers.append(nn.ReLU())\n",
    "            layers.append(nn.Dropout(0.2))\n",
    "            mlp_input_dim = layer_dim\n",
    "        self.mlp = nn.Sequential(*layers)\n",
    "        \n",
    "        # final fully connected layer\n",
    "        self.fc = nn.Linear(gmf_embed_dim+mlp_input_dim, 1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # x = [item_id, user_id]\n",
    "    \n",
    "        # gmf\n",
    "        gmf_embedding = self.gmf_embedding(x)\n",
    "        item_gmf_embedding = gmf_embedding[: ,0]\n",
    "        user_gmf_embedding = gmf_embedding[: ,1]\n",
    "        # element wise multiplication\n",
    "        gmf_out = item_gmf_embedding * user_gmf_embedding\n",
    "        \n",
    "        # mlp \n",
    "        mlp_embedding = self.mlp_embedding(x)\n",
    "        concat_mlp_embedding = mlp_embedding.view(-1,self.mlp_input_dim)\n",
    "        mlp_out = self.mlp(concat_mlp_embedding)\n",
    "        \n",
    "        # concat gmf & mlp output\n",
    "        out = torch.cat([gmf_out, mlp_out], dim=1)\n",
    "        out = self.fc(out)\n",
    "        return torch.sigmoid(out)\n",
    "    \n",
    "    def predict(self, x):\n",
    "        self.eval()\n",
    "        with torch.no_grad():\n",
    "            return self.forward(x) \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, dataloader, epochs=20, lr=0.001):\n",
    "    device = (\n",
    "        torch.device(\"cuda:0\") if torch.cuda.is_available(\n",
    "        ) else torch.device(\"cpu\")\n",
    "    )\n",
    "    model.to(device)\n",
    "    model.train()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    criterion = nn.BCELoss()\n",
    "    training_history = []\n",
    "    for epoch in range(epochs):\n",
    "        epoch_loss = 0\n",
    "        for x, y in dataloader:\n",
    "            y_pred = model.forward(x)\n",
    "            loss = criterion(y_pred, y)\n",
    "            epoch_loss += loss\n",
    "            model.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        epoch_loss /= len(dataloader)\n",
    "        training_history.append(epoch_loss)\n",
    "        if epoch%10 == 0:\n",
    "            print(f\"Epoch {epoch}: {epoch_loss:.4f}\")\n",
    "    return model, training_history"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preparation\n",
    "- X is an array of feature indices: [[item_id, user_id]]\n",
    "- y is just a (n_sample, 1) array of the ground truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "rating, item, user = utils.get_movielens()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/mq/jw5f6khs3976k9gkdxw723_m0000gn/T/ipykernel_58254/1945452404.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X['user_id'] = X['user_id'] + rating['item_id'].max() + 1 # offset\n"
     ]
    }
   ],
   "source": [
    "X = rating[['item_id', 'user_id']]\n",
    "X['user_id'] = X['user_id'] + rating['item_id'].max() + 1 # offset\n",
    "X = X.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert rating to 1/0\n",
    "threshold = 3\n",
    "y = np.where(rating['rating'].to_numpy()>=threshold, 1, 0).reshape(-1, 1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train test split\n",
    "\n",
    "Here, for simplicity, we are only using a random split, with 80% as the train set, and 20% as the test set. In practice, the splitting maybe done by user, e.g. 80/20 split of a user's rating/interaction history."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = data.TensorDataset(torch.from_numpy(X_train), torch.from_numpy(y_train).float())\n",
    "train_dataloader = DataLoader(dataset, batch_size=128, shuffle=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = NCF(\n",
    "    mlp_layers=[32, 16, 8], \n",
    "    n_features=X.max()+1, \n",
    "    gmf_embed_dim=30, \n",
    "    mlp_embed_dim=30, \n",
    "    dropout=0.2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 0.5329\n",
      "Epoch 10: 0.3415\n",
      "Epoch 20: 0.2368\n",
      "Epoch 30: 0.1483\n",
      "Epoch 40: 0.0851\n",
      "Epoch 50: 0.0473\n",
      "Epoch 60: 0.0262\n",
      "Epoch 70: 0.0150\n",
      "Epoch 80: 0.0074\n",
      "Epoch 90: 0.0043\n"
     ]
    }
   ],
   "source": [
    "model, history = train(model=model,\n",
    "                       dataloader=train_dataloader,\n",
    "                       epochs=100,\n",
    "                       lr=0.001)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_soft = model.predict(torch.from_numpy(X_test))\n",
    "y_pred = np.where(y_pred_soft.numpy() > 0.5, 1, 0)\n",
    "\n",
    "acc = accuracy_score(y_pred, y_test)\n",
    "auc = roc_auc_score(y_test, y_pred_soft)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "cf_mat = confusion_matrix(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7627\n",
      "AUC: 0.6320061250614306\n",
      "F1 Score: 0.8562515144172522\n"
     ]
    }
   ],
   "source": [
    "print(f\"Accuracy: {acc}\")\n",
    "print(f\"AUC: {auc}\")\n",
    "print(f\"F1 Score: {f1}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1119,  2349],\n",
       "       [ 2397, 14135]])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cf_mat\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "2d59829e856ecc8da3b928adad583801194efdadc7aa71fa7c39d3d39c851f1a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
