{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Factorization Machines (https://www.csie.ntu.edu.tw/~b97053/paper/Rendle2010FM.pdf)\n",
    "- a generic algorithm for classification, regression and ranking\n",
    "- a generalization of linear regression and matrix factorization model\n",
    "- similar to SVM with a polynomial kernel\n",
    "- models n-way (typically 2-way) interaction between features (embeddings)\n",
    "- compute 2-way feature interaction as the dot product of the latent features\n",
    "- author proposes a neat trick to reduce polynomial complexity to linear time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the model (2-way interaction): \n",
    "\n",
    "$\\hat{y}(x) = \\mathbf{w}_0 + \\sum_{i=1}^d \\mathbf{w}_i x_i + \\sum_{i=1}^d\\sum_{j=i+1}^d \\langle\\mathbf{v}_i, \\mathbf{v}_j\\rangle x_i x_j$\n",
    "\n",
    "- $w_0$ - global bias\n",
    "- $w_i$ - weight of i-th variable\n",
    "- $V, v_i, v_j$ - feature embeddings\n",
    "- $\\langle\\mathbf{v}_i, \\mathbf{v}_j\\rangle$ - dot product of embeddings -> weight of the interaction\n",
    "\n",
    "As we can see, the first part of the equation is just a linear regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "derivation of pairwise interaction\n",
    "\n",
    "\\begin{split}\\begin{aligned}\n",
    "&\\sum_{i=1}^d \\sum_{j=i+1}^d \\langle\\mathbf{v}_i, \\mathbf{v}_j\\rangle x_i x_j \\\\\n",
    " &= \\frac{1}{2} \\sum_{i=1}^d \\sum_{j=1}^d\\langle\\mathbf{v}_i, \\mathbf{v}_j\\rangle x_i x_j - \\frac{1}{2}\\sum_{i=1}^d \\langle\\mathbf{v}_i, \\mathbf{v}_i\\rangle x_i x_i \\\\\n",
    " &= \\frac{1}{2} \\big (\\sum_{i=1}^d \\sum_{j=1}^d \\sum_{l=1}^k\\mathbf{v}_{i, l} \\mathbf{v}_{j, l} x_i x_j - \\sum_{i=1}^d \\sum_{l=1}^k \\mathbf{v}_{i, l} \\mathbf{v}_{i, l} x_i x_i \\big)\\\\\n",
    " &=  \\frac{1}{2} \\sum_{l=1}^k \\big ((\\sum_{i=1}^d \\mathbf{v}_{i, l} x_i) (\\sum_{j=1}^d \\mathbf{v}_{j, l}x_j) - \\sum_{i=1}^d \\mathbf{v}_{i, l}^2 x_i^2 \\big ) \\\\\n",
    " &= \\frac{1}{2} \\sum_{l=1}^k \\big ((\\sum_{i=1}^d \\mathbf{v}_{i, l} x_i)^2 - \\sum_{i=1}^d \\mathbf{v}_{i, l}^2 x_i^2)\n",
    " \\end{aligned}\\end{split}\n",
    "\n",
    " - $k$ - embedding dimension\n",
    " - $d$ - number of features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An illustration of the shape & format of the data, where each feature will be embedded into a latent space\n",
    "\n",
    "<img src=\"illustration.png\" style=\"width:600px;height:300px;\">\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils import data\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, f1_score, confusion_matrix\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Factorization Machines can be very easily implemented in Pytorch with just a few lines of code!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "class FM(nn.Module):\n",
    "    def __init__(self, n_features, embed_dim):\n",
    "        super().__init__()\n",
    "        self.feature_embedding = nn.Embedding(n_features, embed_dim) # n_dim feature embeddings for 2-way interaction\n",
    "        torch.nn.init.xavier_uniform(self.feature_embedding.weight)\n",
    "        self.fc = nn.Embedding(n_features, 1) # 1-d embedding equivalent to the weights for each feature in linear regression\n",
    "        self.bias = nn.Parameter(torch.zeros((1,))) # global bias term\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        first_order = self.fc(x).sum(dim=1) + self.bias\n",
    "        second_order = self.factorization_machine(self.feature_embedding(x))\n",
    "        res = first_order + second_order\n",
    "        return torch.sigmoid(res)\n",
    "    \n",
    "    \n",
    "    def factorization_machine(self, x_embed):\n",
    "        \"\"\"compute the 2-way interaction term\n",
    "        \"\"\"\n",
    "        sq_sum = x_embed.sum(dim=1)**2 # (x_size, n_dim)\n",
    "        sum_sq = (x_embed**2).sum(dim=1) # (x_size, n_dim)\n",
    "        return 0.5 * (sq_sum - sum_sq).sum(dim=1, keepdim=True) # (x_size, 1)\n",
    "    \n",
    "    def predict(self, x):\n",
    "        self.eval()\n",
    "        with torch.no_grad():\n",
    "            return self.forward(x)\n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `factorization_machine()` function implements the equation:\n",
    "\n",
    "$\\frac{1}{2} \\sum_{l=1}^k \\big ((\\sum_{i=1}^d \\mathbf{v}_{i, l} x_i)^2 - \\sum_{i=1}^d \\mathbf{v}_{i, l}^2 x_i^2)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are two parts in the equation:\n",
    "\n",
    "$(\\mathbf{v}_{i, l} x_i)^2$     and     $(\\mathbf{v}_{i, l}^2 x_i^2)$\n",
    "\n",
    "Since each $x_i$ corresponds to one $v_i$, we can just treat $\\mathbf{v}_{i, l} x_i$ as one latent embedding vector, which will be estimated during training, instead of estimating $V$ separately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train loop\n",
    "def train(model, dataloader, epochs=20, lr=0.001):\n",
    "    device = (\n",
    "        torch.device(\"cuda:0\") if torch.cuda.is_available(\n",
    "        ) else torch.device(\"cpu\")\n",
    "    )\n",
    "    model.to(device)\n",
    "    model.train()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    criterion = nn.BCELoss()\n",
    "    training_history = []\n",
    "    for epoch in range(epochs):\n",
    "        epoch_loss = 0\n",
    "        for x, y in dataloader:\n",
    "            y_pred = model.forward(x)\n",
    "            loss = criterion(y_pred, y)\n",
    "            epoch_loss += loss\n",
    "            model.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        epoch_loss /= len(dataloader)\n",
    "        training_history.append(epoch_loss)\n",
    "        if epoch%10 == 0:\n",
    "            print(f\"Epoch {epoch}: {epoch_loss:.4f}\")\n",
    "    return model, training_history\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preparation\n",
    "- X is an array of feature indices (n_samples, n_attributes), where each feature index will be mapped to a latent factor\n",
    "  - [user, item, user_features, item_features]\n",
    "- y is just a (n_sample, 1) array of the ground truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "rating, item, user = utils.get_movielens()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "item_label = utils.get_items_label_encoding(item, return_df=False)\n",
    "user_label = utils.get_users_label_encoding(user, return_df=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# concat item & user feature matrix to get X\n",
    "X = np.hstack((item_label[rating['item_id']-1,:], user_label[rating['user_id']-1,:])) # offset -1 since item&user id starts with 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert rating to 1/0\n",
    "threshold = 3\n",
    "y = np.where(rating['rating'].to_numpy()>=threshold, 1, 0).reshape(-1, 1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train test split\n",
    "\n",
    "Here, for simplicity, we are only using a random split, with 80% as the train set, and 20% as the test set. In practice, the splitting maybe done by user, e.g. 80/20 split of a user's rating/interaction history."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = data.TensorDataset(torch.from_numpy(X_train), torch.from_numpy(y_train).float())\n",
    "train_dataloader = DataLoader(dataset, batch_size=128, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/mq/jw5f6khs3976k9gkdxw723_m0000gn/T/ipykernel_13209/509149522.py:7: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
      "  torch.nn.init.xavier_uniform(self.feature_embedding.weight)\n"
     ]
    }
   ],
   "source": [
    "n_features = X.max() + 1\n",
    "embed_dim = 20\n",
    "model = FM(n_features=n_features, embed_dim=embed_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 0.4881\n",
      "Epoch 10: 0.2507\n",
      "Epoch 20: 0.2117\n",
      "Epoch 30: 0.1971\n",
      "Epoch 40: 0.1823\n",
      "Epoch 50: 0.1801\n",
      "Epoch 60: 0.1749\n",
      "Epoch 70: 0.1717\n",
      "Epoch 80: 0.1741\n",
      "Epoch 90: 0.1692\n",
      "Epoch 100: 0.1709\n",
      "Epoch 110: 0.1706\n",
      "Epoch 120: 0.1662\n",
      "Epoch 130: 0.1680\n",
      "Epoch 140: 0.1711\n",
      "Epoch 150: 0.1712\n",
      "Epoch 160: 0.1724\n",
      "Epoch 170: 0.1710\n",
      "Epoch 180: 0.1688\n",
      "Epoch 190: 0.1791\n"
     ]
    }
   ],
   "source": [
    "model, history = train(model, train_dataloader, epochs=200, lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_soft = model.predict(torch.from_numpy(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.76355"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = np.where(y_pred_soft.numpy() > 0.5, 1, 0)\n",
    "accuracy_score(y_pred, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6677428361646534"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "roc_auc_score(y_test, y_pred_soft)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8549608955681643"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1333,  2135],\n",
       "       [ 2594, 13938]])"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(y_test, y_pred)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "2d59829e856ecc8da3b928adad583801194efdadc7aa71fa7c39d3d39c851f1a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
