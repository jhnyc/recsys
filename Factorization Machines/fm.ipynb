{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Factorization Machines (https://www.csie.ntu.edu.tw/~b97053/paper/Rendle2010FM.pdf)\n",
    "- a generic algorithm for classification, regression and ranking\n",
    "- a generalization of linear regression and matrix factorization model\n",
    "- similar to SVM with a polynomial kernel\n",
    "- models n-way (typically 2-way) interaction between features (embeddings)\n",
    "- compute 2-way feature interaction as the dot product of the latent features\n",
    "- author proposes a neat trick to reduce polynomial complexity to linear time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the model (2-way interaction): \n",
    "\n",
    "$\\hat{y}(x) = \\mathbf{w}_0 + \\sum_{i=1}^d \\mathbf{w}_i x_i + \\sum_{i=1}^d\\sum_{j=i+1}^d \\langle\\mathbf{v}_i, \\mathbf{v}_j\\rangle x_i x_j$\n",
    "\n",
    "- $w_0$ - global bias\n",
    "- $w_i$ - weight of i-th variable\n",
    "- $V, v_i, v_j$ - feature embeddings\n",
    "- $\\langle\\mathbf{v}_i, \\mathbf{v}_j\\rangle$ - dot product of embeddings -> weight of the interaction\n",
    "\n",
    "As we can see, the first part of the equation is just a linear regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "derivation of pairwise interaction\n",
    "\n",
    "\\begin{split}\\begin{aligned}\n",
    "&\\sum_{i=1}^d \\sum_{j=i+1}^d \\langle\\mathbf{v}_i, \\mathbf{v}_j\\rangle x_i x_j \\\\\n",
    " &= \\frac{1}{2} \\sum_{i=1}^d \\sum_{j=1}^d\\langle\\mathbf{v}_i, \\mathbf{v}_j\\rangle x_i x_j - \\frac{1}{2}\\sum_{i=1}^d \\langle\\mathbf{v}_i, \\mathbf{v}_i\\rangle x_i x_i \\\\\n",
    " &= \\frac{1}{2} \\big (\\sum_{i=1}^d \\sum_{j=1}^d \\sum_{l=1}^k\\mathbf{v}_{i, l} \\mathbf{v}_{j, l} x_i x_j - \\sum_{i=1}^d \\sum_{l=1}^k \\mathbf{v}_{i, l} \\mathbf{v}_{i, l} x_i x_i \\big)\\\\\n",
    " &=  \\frac{1}{2} \\sum_{l=1}^k \\big ((\\sum_{i=1}^d \\mathbf{v}_{i, l} x_i) (\\sum_{j=1}^d \\mathbf{v}_{j, l}x_j) - \\sum_{i=1}^d \\mathbf{v}_{i, l}^2 x_i^2 \\big ) \\\\\n",
    " &= \\frac{1}{2} \\sum_{l=1}^k \\big ((\\sum_{i=1}^d \\mathbf{v}_{i, l} x_i)^2 - \\sum_{i=1}^d \\mathbf{v}_{i, l}^2 x_i^2)\n",
    " \\end{aligned}\\end{split}\n",
    "\n",
    " - $k$ - embedding dimension\n",
    " - $d$ - number of features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An illustration of the shape & format of the data, where each feature will be embedded into a latent space\n",
    "\n",
    "<img src=\"illustration.png\" style=\"width:600px;height:300px;\">\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils import data\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Factorization Machines can be very easily implemented in Pytorch with just a few lines of code!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "class FM(nn.Module):\n",
    "    def __init__(self, n_features, embed_dim):\n",
    "        super().__init__()\n",
    "        self.feature_embedding = nn.Embedding(n_features, embed_dim) # n_dim feature embeddings for 2-way interaction\n",
    "        self.fc = nn.Embedding(n_features, 1) # 1-d embedding equivalent to the weights for each feature in linear regression\n",
    "        self.bias = nn.Parameter(torch.zeros((1,))) # global bias term\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        first_order = self.fc(x).sum(dim=1) + self.bias\n",
    "        second_order = self.factorization_machine(self.feature_embedding(x))\n",
    "        res = first_order + second_order\n",
    "        return res\n",
    "    \n",
    "    \n",
    "    def factorization_machine(self, x_embed):\n",
    "        \"\"\"compute the 2-way interaction term\n",
    "        \"\"\"\n",
    "        sq_sum = x_embed.sum(dim=1)**2 # (x_size, n_dim)\n",
    "        sum_sq = (x_embed**2).sum(dim=1) # (x_size, n_dim)\n",
    "        return 0.5 * (sq_sum - sum_sq).sum(dim=1, keepdim=True) # (x_size, 1)\n",
    "    \n",
    "    def predict(self, x):\n",
    "        self.eval()\n",
    "        with torch.no_grad():\n",
    "            return self.forward(x)\n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `factorization_machine()` function implements the equation:\n",
    "\n",
    "$\\frac{1}{2} \\sum_{l=1}^k \\big ((\\sum_{i=1}^d \\mathbf{v}_{i, l} x_i)^2 - \\sum_{i=1}^d \\mathbf{v}_{i, l}^2 x_i^2)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are two parts in the equation:\n",
    "\n",
    "$(\\mathbf{v}_{i, l} x_i)^2$     and     $(\\mathbf{v}_{i, l}^2 x_i^2)$\n",
    "\n",
    "Since each $x_i$ corresponds to one $v_i$, we can just treat $\\mathbf{v}_{i, l} x_i$ as one latent embedding vector, which will be estimated during training, instead of estimating $V$ separately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train loop\n",
    "def train(model, dataloader, epochs=20, lr=0.001):\n",
    "    device = (\n",
    "        torch.device(\"cuda:0\") if torch.cuda.is_available(\n",
    "        ) else torch.device(\"cpu\")\n",
    "    )\n",
    "    model.to(device)\n",
    "    model.train()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    criterion = nn.MSELoss()\n",
    "    training_history = []\n",
    "    for epoch in range(epochs):\n",
    "        epoch_loss = 0\n",
    "        for x, y in dataloader:\n",
    "            y_pred = model.forward(x)\n",
    "            loss = criterion(y_pred, y)\n",
    "            epoch_loss += loss\n",
    "            model.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        epoch_loss /= len(dataloader)\n",
    "        training_history.append(epoch_loss)\n",
    "        if epoch%10 == 0:\n",
    "            print(f\"Epoch {epoch}: {epoch_loss:.4f}\")\n",
    "    return model, training_history\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preparation\n",
    "- X is an array of feature indices (n_samples, n_attributes), where each feature index will be mapped to a latent factor\n",
    "  - [user, item, user_features, item_features]\n",
    "- y is just a (n_sample, 1) array of the ground truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "movie, rating = utils.load_movielens()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Encode movie features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# construct an array of feature indices for each item\n",
    "all_movie_features = set([f for feat in movie['features'] for f in feat])\n",
    "feature_to_id = {f:ix for ix, f in enumerate(all_movie_features)}\n",
    "movie_feat_id = movie['features'].apply(lambda x: [feature_to_id[f]+1 for f in x]).to_list() # +1 since 0 is the null feature\n",
    "\n",
    "# since items have variable # of features, pad feature sequence with 0 \n",
    "features_enc = pad_sequence([torch.tensor(i) for i in movie_feat_id], batch_first=True, padding_value=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Combine userId, movieId and features to get our `x`\n",
    "- [userId, movieId, feature-1, feature-n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_user = rating['userId'].nunique()\n",
    "n_movies = movie['movieId'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[   0,   30,    2, ...,    0,    0,    0],\n",
       "       [   0,  833,   24, ...,    0,    0,    0],\n",
       "       [   0,  859,   10, ...,    0,    0,    0],\n",
       "       ...,\n",
       "       [ 670, 4603,    3, ...,    0,    0,    0],\n",
       "       [ 670, 4616,    2, ...,    0,    0,    0],\n",
       "       [ 670, 4703,    2, ...,    0,    0,    0]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# join userid & movieid, and features\n",
    "x = rating[['userId', 'movieId']].to_numpy()\n",
    "features = features_enc[rating['movieId']]\n",
    "x = np.hstack((x, features))\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[   0,  701, 9798, ..., 9796, 9796, 9796],\n",
       "       [   0, 1504, 9820, ..., 9796, 9796, 9796],\n",
       "       [   0, 1530, 9806, ..., 9796, 9796, 9796],\n",
       "       ...,\n",
       "       [ 670, 5274, 9799, ..., 9796, 9796, 9796],\n",
       "       [ 670, 5287, 9798, ..., 9796, 9796, 9796],\n",
       "       [ 670, 5374, 9798, ..., 9796, 9796, 9796]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# calculate offset, avoid duplicated feature indices\n",
    "x[:, 1] += n_user\n",
    "x[:, 2:] += n_user+n_movies\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = rating['rating'].to_numpy().reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create train & test set\n",
    "- train: first n-1 ratings per user (n is the number of ratings of the user)\n",
    "- test: last/most recent rating per user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_ix = [i for i,v in enumerate(x[:-1, 0]) if v != x[i+1,0]] + [len(x)-1]\n",
    "train_ix = [i for i in range(len(x)) if i not in test_ix]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test = x[train_ix], x[test_ix]\n",
    "y_train, y_test = y[train_ix], y[test_ix]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = data.TensorDataset(torch.from_numpy(x_train), torch.from_numpy(y_train).float())\n",
    "train_dataloader = DataLoader(dataset, batch_size=128, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features.shape[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_features = n_user + n_movies + len(all_movie_features) + 1 # +1 for the null feature\n",
    "embed_dim = 20\n",
    "\n",
    "model = FM(n_features=n_features, embed_dim=embed_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 98063.4766\n",
      "Epoch 10: 78.0661\n",
      "Epoch 20: 7.3351\n",
      "Epoch 30: 1.5365\n",
      "Epoch 40: 0.7909\n",
      "Epoch 50: 0.6228\n",
      "Epoch 60: 0.5518\n",
      "Epoch 70: 0.5039\n",
      "Epoch 80: 0.4661\n",
      "Epoch 90: 0.4333\n",
      "Epoch 100: 0.4006\n",
      "Epoch 110: 0.3723\n",
      "Epoch 120: 0.3476\n",
      "Epoch 130: 0.3234\n",
      "Epoch 140: 0.3024\n",
      "Epoch 150: 0.2840\n",
      "Epoch 160: 0.2689\n",
      "Epoch 170: 0.2552\n",
      "Epoch 180: 0.2414\n",
      "Epoch 190: 0.2301\n",
      "Epoch 200: 0.2197\n",
      "Epoch 210: 0.2112\n",
      "Epoch 220: 0.2035\n",
      "Epoch 230: 0.1972\n",
      "Epoch 240: 0.1901\n",
      "Epoch 250: 0.1849\n",
      "Epoch 260: 0.1797\n",
      "Epoch 270: 0.1748\n",
      "Epoch 280: 0.1703\n",
      "Epoch 290: 0.1674\n"
     ]
    }
   ],
   "source": [
    "model, history = train(model, train_dataloader, epochs=300, lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(torch.from_numpy(x_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.4924757863411653"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_squared_error(y_pred, y_test, squared=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.7903649881593162"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "mean_absolute_error(y_pred, y_test)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "2d59829e856ecc8da3b928adad583801194efdadc7aa71fa7c39d3d39c851f1a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
