{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reference: \n",
    "- https://d2l.ai/chapter_recommender-systems/fm.html\n",
    "- https://github.com/khanhnamle1994/MetaRec/blob/master/Matrix-Factorization-Experiments/Factorization-Machines/FM.py\n",
    "- https://github.com/qian135/ctr_model_zoo/blob/master/common.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Original paper:\n",
    "https://www.csie.ntu.edu.tw/~b97053/paper/Rendle2010FM.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Factorization Machines\n",
    "- a generic algorithm for classification, regression and ranking\n",
    "- a generalization of linear regression and matrix factorization model\n",
    "- similar to SVM with a polynomial kernel\n",
    "- models n-way (typically 2-way) interaction between features (embeddings)\n",
    "- compute 2-way feature interaction as the dot product of the latent features\n",
    "- author proposes a neat trick to reduce polynomial complexity to linear time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the model (2-way interaction): \n",
    "\n",
    "$\\hat{y}(x) = \\mathbf{w}_0 + \\sum_{i=1}^d \\mathbf{w}_i x_i + \\sum_{i=1}^d\\sum_{j=i+1}^d \\langle\\mathbf{v}_i, \\mathbf{v}_j\\rangle x_i x_j$\n",
    "\n",
    "- $w_0$ - global bias\n",
    "- $w_i$ - weight of i-th variable\n",
    "- $V, v_i, v_j$ - feature embeddings\n",
    "- $\\langle\\mathbf{v}_i, \\mathbf{v}_j\\rangle$ - dot product of embeddings -> weight of the interaction\n",
    "\n",
    "As we can see, the first part of the equation is just a linear regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "derivation of pairwise interaction\n",
    "\n",
    "\\begin{split}\\begin{aligned}\n",
    "&\\sum_{i=1}^d \\sum_{j=i+1}^d \\langle\\mathbf{v}_i, \\mathbf{v}_j\\rangle x_i x_j \\\\\n",
    " &= \\frac{1}{2} \\sum_{i=1}^d \\sum_{j=1}^d\\langle\\mathbf{v}_i, \\mathbf{v}_j\\rangle x_i x_j - \\frac{1}{2}\\sum_{i=1}^d \\langle\\mathbf{v}_i, \\mathbf{v}_i\\rangle x_i x_i \\\\\n",
    " &= \\frac{1}{2} \\big (\\sum_{i=1}^d \\sum_{j=1}^d \\sum_{l=1}^k\\mathbf{v}_{i, l} \\mathbf{v}_{j, l} x_i x_j - \\sum_{i=1}^d \\sum_{l=1}^k \\mathbf{v}_{i, l} \\mathbf{v}_{i, l} x_i x_i \\big)\\\\\n",
    " &=  \\frac{1}{2} \\sum_{l=1}^k \\big ((\\sum_{i=1}^d \\mathbf{v}_{i, l} x_i) (\\sum_{j=1}^d \\mathbf{v}_{j, l}x_j) - \\sum_{i=1}^d \\mathbf{v}_{i, l}^2 x_i^2 \\big ) \\\\\n",
    " &= \\frac{1}{2} \\sum_{l=1}^k \\big ((\\sum_{i=1}^d \\mathbf{v}_{i, l} x_i)^2 - \\sum_{i=1}^d \\mathbf{v}_{i, l}^2 x_i^2)\n",
    " \\end{aligned}\\end{split}\n",
    "\n",
    " - $k$ - embedding dimension\n",
    " - $d$ - number of features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An illustration of the shape & format of the data, where each feature will be embedded into a latent space\n",
    "\n",
    "<img src=\"data_illustration.png\" style=\"width:600px;height:300px;\">\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils import data\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Factorization Machines can be very easily implemented in Pytorch with just a few lines of code!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "class FM(nn.Module):\n",
    "    def __init__(self, n_features, embed_dim):\n",
    "        super().__init__()\n",
    "        self.feature_embedding = nn.Embedding(n_features, embed_dim) # n_dim feature embeddings for 2-way interaction\n",
    "        self.fc = nn.Embedding(n_features, 1) # 1-d embedding equivalent to the weights for each feature in linear regression\n",
    "        self.bias = nn.Parameter(torch.zeros((1,))) # global bias term\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        first_order = self.fc(x).sum(dim=1) + self.bias\n",
    "        second_order = self.factorization_machine(self.feature_embedding(x))\n",
    "        res = first_order + second_order\n",
    "        return res\n",
    "    \n",
    "    \n",
    "    def factorization_machine(self, x_embed):\n",
    "        \"\"\"compute the 2-way interaction term\n",
    "        \"\"\"\n",
    "        sq_sum = x_embed.sum(dim=1)**2 # (x_size, n_dim)\n",
    "        sum_sq = (x_embed**2).sum(dim=1) # (x_size, n_dim)\n",
    "        return 0.5 * (sq_sum - sum_sq).sum(dim=1, keepdim=True) # (x_size, 1)\n",
    "    \n",
    "    def predict(self, x):\n",
    "        self.eval()\n",
    "        with torch.no_grad():\n",
    "            return self.forward(x)\n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `factorization_machine()` function implements the equation:\n",
    "\n",
    "$\\frac{1}{2} \\sum_{l=1}^k \\big ((\\sum_{i=1}^d \\mathbf{v}_{i, l} x_i)^2 - \\sum_{i=1}^d \\mathbf{v}_{i, l}^2 x_i^2)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are two parts in the equation:\n",
    "\n",
    "$(\\mathbf{v}_{i, l} x_i)^2$     and     $(\\mathbf{v}_{i, l}^2 x_i^2)$\n",
    "\n",
    "Since each $x_i$ corresponds to one $v_i$, we can just treat $\\mathbf{v}_{i, l} x_i$ as one latent embedding vector, which will be estimated during training, instead of estimating $V$ separately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train loop\n",
    "def train(model, dataloader, epochs=20, lr=0.001):\n",
    "    device = (\n",
    "        torch.device(\"cuda:0\") if torch.cuda.is_available(\n",
    "        ) else torch.device(\"cpu\")\n",
    "    )\n",
    "    model.to(device)\n",
    "    model.train()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    criterion = nn.MSELoss()\n",
    "    training_history = []\n",
    "    for epoch in range(epochs):\n",
    "        epoch_loss = 0\n",
    "        for x, y in dataloader:\n",
    "            y_pred = model.forward(x)\n",
    "            loss = criterion(y_pred, y)\n",
    "            epoch_loss += loss\n",
    "            model.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        epoch_loss /= len(dataloader)\n",
    "        training_history.append(epoch_loss)\n",
    "        if epoch%10 == 0:\n",
    "            print(f\"Epoch {epoch}: {epoch_loss:.4f}\")\n",
    "    return model, training_history\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preparation\n",
    "- X is an array of feature indices (n_samples, n_attributes), where each feature index will be mapped to a latent factor\n",
    "  - [user, item, user_features, item_features]\n",
    "- y is just a (n_sample, 1) array of the ground truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "movie, rating = utils.load_movielens()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Encode movie features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0,  2,  4, ..., 58, 60, 62],\n",
       "       [ 0,  2,  4, ..., 58, 60, 62],\n",
       "       [ 0,  2,  4, ..., 58, 60, 62],\n",
       "       ...,\n",
       "       [ 0,  2,  4, ..., 58, 60, 62],\n",
       "       [ 0,  2,  4, ..., 58, 60, 62],\n",
       "       [ 0,  2,  4, ..., 58, 60, 62]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "enc = MultiLabelBinarizer()\n",
    "multi_hot = enc.fit_transform([set([str(j) for j in i]) for i in movie['features']])\n",
    "\n",
    "# convert multi_hot_encoding to indices\n",
    "# e.g. [0, 2, 5] -> 1st & 2nd features are absent, 3rd feature is present\n",
    "features_enc = multi_hot + np.arange(multi_hot.shape[1])*2\n",
    "features_enc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Combine userId, movieId and features to get our `x`\n",
    "- [userId, movieId, feature-1, feature-n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_user = rating['userId'].nunique()\n",
    "n_movies = movie['movieId'].nunique()\n",
    "n_movie_features = features_enc.shape[-1]*2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[   0,   30,    0, ...,   58,   60,   62],\n",
       "       [   0,  833,    0, ...,   58,   60,   62],\n",
       "       [   0,  859,    0, ...,   59,   60,   62],\n",
       "       ...,\n",
       "       [ 670, 4603,    0, ...,   59,   60,   62],\n",
       "       [ 670, 4616,    0, ...,   58,   60,   62],\n",
       "       [ 670, 4703,    0, ...,   58,   60,   62]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = rating[['userId', 'movieId']].to_numpy()\n",
    "features = features_enc[rating['movieId']]\n",
    "\n",
    "# concat userid, movieid, and features to get x\n",
    "x = np.hstack((x, features))\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[   0,  701, 9796, ..., 9854, 9856, 9858],\n",
       "       [   0, 1504, 9796, ..., 9854, 9856, 9858],\n",
       "       [   0, 1530, 9796, ..., 9855, 9856, 9858],\n",
       "       ...,\n",
       "       [ 670, 5274, 9796, ..., 9855, 9856, 9858],\n",
       "       [ 670, 5287, 9796, ..., 9854, 9856, 9858],\n",
       "       [ 670, 5374, 9796, ..., 9854, 9856, 9858]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# calculate offset, avoid duplicated feature indices\n",
    "x[:, 1] += n_user\n",
    "x[:, 2:] += n_user+n_movies\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = rating['rating'].to_numpy().reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create train & test set\n",
    "- train: first n-1 ratings per user (n is the number of ratings of the user)\n",
    "- test: last/most recent rating per user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_ix = [i for i,v in enumerate(x[:-1, 0]) if v != x[i+1,0]] + [len(x)-1]\n",
    "train_ix = [i for i in range(len(x)) if i not in test_ix]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test = x[train_ix], x[test_ix]\n",
    "y_train, y_test = y[train_ix], y[test_ix]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = data.TensorDataset(torch.from_numpy(x_train), torch.from_numpy(y_train).float())\n",
    "train_dataloader = DataLoader(dataset, batch_size=128, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_features = n_user + n_movies + n_movie_features*2\n",
    "embed_dim = 40\n",
    "\n",
    "model = FM(n_features=n_features, embed_dim=embed_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 2158.6504\n",
      "Epoch 10: 55.0315\n",
      "Epoch 20: 19.3444\n",
      "Epoch 30: 9.8613\n",
      "Epoch 40: 5.8314\n",
      "Epoch 50: 3.8517\n",
      "Epoch 60: 2.7472\n",
      "Epoch 70: 2.2466\n",
      "Epoch 80: 1.8223\n",
      "Epoch 90: 1.5533\n",
      "Epoch 100: 1.4293\n",
      "Epoch 110: 1.2750\n",
      "Epoch 120: 1.2069\n",
      "Epoch 130: 1.1149\n",
      "Epoch 140: 1.0458\n",
      "Epoch 150: 0.9703\n",
      "Epoch 160: 0.8981\n",
      "Epoch 170: 0.8504\n",
      "Epoch 180: 0.8140\n",
      "Epoch 190: 0.7846\n"
     ]
    }
   ],
   "source": [
    "model, history = train(model, train_dataloader, epochs=200, lr=0.001)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "2d59829e856ecc8da3b928adad583801194efdadc7aa71fa7c39d3d39c851f1a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
