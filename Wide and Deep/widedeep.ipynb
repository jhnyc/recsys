{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wide and Deep Learning (https://arxiv.org/pdf/1606.07792.pdf)\n",
    "\n",
    "- a \"wide\" component that uses embedding layers to represent users and items\n",
    "  - wide linear layer to learn interaction & co-occurence between features\n",
    "- a \"deep\" component that uses fully-connected layers to learn the interactions between users and items\n",
    "  - embedding based MLP to generalize to unseen item feature pairs\n",
    "  - deep learning to model complex feature interactions\n",
    "- The final output is the sum of the outputs from the wide and deep components.\n",
    "- The idea of Wide and Deep Learning is to achieve both **memorization** and **generalization**\n",
    "- Focus on ranking of items"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"model_illustration.png\" style=\"width:700px;\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils import data\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, top_k_accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WideDeep(nn.Module):\n",
    "    def __init__(self, n_fields, n_features, mlp_dims=[256, 128, 64], embed_dim=64, dropout=0.2):\n",
    "        super().__init__()\n",
    "        # wide linear component\n",
    "        self.linear = nn.Embedding(n_features, 1)\n",
    "        self.bias = nn.Parameter(torch.zeros((1,)))\n",
    "        # deep mlp component\n",
    "        self.embedding = nn.Embedding(n_features, embed_dim)\n",
    "        self.embed_out_dim = n_fields*embed_dim\n",
    "        mlp_layers = []\n",
    "        input_dim = self.embed_out_dim \n",
    "        for dim in mlp_dims:\n",
    "            mlp_layers.append(nn.Linear(input_dim, dim))\n",
    "            mlp_layers.append(nn.BatchNorm1d(dim))\n",
    "            mlp_layers.append(nn.ReLU())\n",
    "            mlp_layers.append(nn.Dropout(dropout))\n",
    "            input_dim = dim\n",
    "        mlp_layers.append(nn.Linear(input_dim, 1)) # final output layer\n",
    "        self.mlp = nn.Sequential(*mlp_layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x_embed = self.embedding(x).view(-1, self.embed_out_dim)\n",
    "        x = self.bias + self.linear(x).sum(dim=1) + self.mlp(x_embed)\n",
    "        return torch.sigmoid(x)\n",
    "    \n",
    "    def predict(self, x):\n",
    "        self.eval()\n",
    "        with torch.no_grad():\n",
    "            return self.forward(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model,  dataloader, epochs=20, lr=0.001):\n",
    "    device = (\n",
    "        torch.device(\"cuda:0\") if torch.cuda.is_available(\n",
    "        ) else torch.device(\"cpu\")\n",
    "    )\n",
    "    model.to(device)\n",
    "    model.train()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    criterion = nn.BCELoss()\n",
    "    training_history = []\n",
    "    for epoch in range(epochs):\n",
    "        epoch_loss = 0\n",
    "        for x, y in dataloader:\n",
    "            y_pred = model.forward(x)\n",
    "            loss = criterion(y_pred, y)\n",
    "            epoch_loss += loss\n",
    "            model.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        epoch_loss /= len(dataloader)\n",
    "        training_history.append(epoch_loss)\n",
    "        if epoch%10 == 0:\n",
    "            print(f\"Epoch {epoch}: {epoch_loss:.4f}\")\n",
    "    return model, training_history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preparation\n",
    "- X is an array of feature indices (n_samples, n_attributes), where each feature index will be mapped to a latent factor\n",
    "  - [user, item, user_features, item_features]\n",
    "- y is just a (n_sample, 1) array of the ground truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "rating, item, user = utils.get_movielens()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "item_label = utils.get_items_label_encoding(item, return_df=False)\n",
    "user_label = utils.get_users_label_encoding(user, return_df=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# concat item & user feature matrix to get X\n",
    "user_offset = item_label.max() + 1 \n",
    "user_label_offset = user_label + user_offset\n",
    "X = np.hstack((item_label[rating['item_id']-1,:], user_label_offset[rating['user_id']-1,:])) # offset -1 since item&user id starts with 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert rating to 1/0\n",
    "threshold = 3\n",
    "y = np.where(rating['rating'].to_numpy()>=threshold, 1, 0).reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train test split\n",
    "\n",
    "Here, for simplicity, we are only using a random split, with 80% as the train set, and 20% as the test set. In practice, the splitting maybe done by user, e.g. 80/20 split of a user's rating/interaction history."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = data.TensorDataset(torch.from_numpy(X_train), torch.from_numpy(y_train).float())\n",
    "train_dataloader = DataLoader(dataset, batch_size=128, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_features = X.max() + 1\n",
    "n_fields = X.shape[1]\n",
    "embed_dim = 30\n",
    "\n",
    "model = WideDeep(n_fields=n_fields, \n",
    "                 n_features=n_features, \n",
    "                 mlp_dims=[128, 64], \n",
    "                 embed_dim=embed_dim, \n",
    "                 dropout=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 0.9950\n",
      "Epoch 10: 0.3451\n",
      "Epoch 20: 0.3093\n",
      "Epoch 30: 0.2741\n",
      "Epoch 40: 0.2416\n",
      "Epoch 50: 0.2189\n",
      "Epoch 60: 0.1993\n",
      "Epoch 70: 0.1838\n",
      "Epoch 80: 0.1683\n",
      "Epoch 90: 0.1572\n"
     ]
    }
   ],
   "source": [
    "model, history = train(model, train_dataloader, epochs=100, lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(torch.from_numpy(X_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.80695"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(np.where(y_pred.numpy() > 0.5, 1, 0), y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7676053480984486"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "roc_auc_score(y_test, y_pred)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "2d59829e856ecc8da3b928adad583801194efdadc7aa71fa7c39d3d39c851f1a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
